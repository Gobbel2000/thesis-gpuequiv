\section{Parallelizing the Algorithm}

At the center of this contribution is a parallel GPU-implementation%
\footnote{The code can be found at \url{https://github.com/Gobbel2000/gpuequiv}}
of the energy game introduced in Section~\ref{sec:energy_games}.
Given a game graph as input, it cal\-cu\-lates for each position the energy budgets
required for the attacker, starting at the current position, to win the game.
This algorithm was described in~\cite{bisping2023process} as part of the
spectroscopy algorithm, as well as in~\cite{brihaye2023multi} by T. Brihaye and
A. Goeminne, which more generally covers
\enquote{Multi-Weighted Reachability Games},
but presents essentially the same algorithm structure.

% First, definitions that are also used outside the algorithm
\SetKwData{Updated}{updated}
\SetKwData{Energies}{energies}
\begin{algorithm}[ht]
    \DontPrintSemicolon
    \SetKwData{Start}{start}
    \SetKwData{Visit}{visit}
    \SetKwData{New}{new\_energies}
    \SetKwFor{pFor}{for parallel}{do}{end}

    $\Start = \{g \in G_d \mid \mathrm{Succ}(g) = \emptyset\}$\;
    \lFor{$g \in \Start$}{$\Energies[g] = \{\mathbf{0}\}$}
    \lFor{$g \in G \setminus \Start$}{$\Energies[g] = \emptyset$}
    $\Visit = \bigcup_{g \in \Start} \mathrm{Pred}(g)$\;

    \BlankLine
    \While{$\Visit \neq \emptyset$}{
        \pFor{$g \in \Visit$}{
            $\Visit = \Visit \setminus \{g\}$\;
            \For{$g' \in \mathrm{Succ}(g)$} {
                $\Updated[g'] = \{ \mathsf{upd}^{-1}(e, w(g, g')) \mid e \in \Energies[g'] \}$
            }

            \BlankLine
            \eIf(\tcp*[h]{attack positions}){$g \in G_a$}{
                $\New = \mathrm{Min} \left(
                    \bigcup_{g' \in \mathrm{Succ}(g)} \uparrow \Updated[g']
                \right)$
            } (\tcp*[h]{defend positions}) {
                $\New = \mathrm{Min} \left(
                    \bigcap_{g' \in \mathrm{Succ}(g)} \uparrow \Updated[g']
                \right)$
            }

            \BlankLine
            \If{$\New \neq \Energies[g]$}{
                $\Energies[g] = \New$\;
                $\Visit = \Visit \cup \mathrm{Pred}(g)$\;
            }
        }
    }
    \Return{\Energies}

    \caption{Parallel Energy Game}\label{alg:energy_game}
\end{algorithm}

The basic structure is as shown in Algorithm~\ref{alg:energy_game}.
In order to calculate all the winning budgets,
we start at the end of the game graph and work our way backwards.
At all defend positions with no successors the attacker immediately wins,
so we can set the energies required for the attacker to win there to the
0-valued energy tuple (lines 1--3).
In every iteration a set of positions is visited which updates its associated
energies.
The next iteration visits the predecessors of all positions, whose energies
have changed in the previous iteration, thus walking backwards in the game
graph in a breadth-first manner. Once an iteration caused no changes to the
energies, the algorithm terminates.

We achieve parallelization by visiting all positions in the visit list in
parallel, but to fully take advantage of the massively multithreaded
GPU-architecture, each position is further processed by multiple threads.

The core of the algorithm lies in lines 11--15, where the upwards-closed sets of
energies are either unioned or intersected.
Since both operations demand very different approaches,
handling them in a single shader would inevitably incur high branch-divergence.
Therefore both cases are handled by separate, more regular shaders:
one for processing attack positions, another for defend positions.
This technique is known as \enquote{Kernel fission}
and improves resource utilization by allowing threads in a work group to follow
similar instruction paths~\cite{Hijma2023}.
The following two sections detail how these operations have been implemented to
efficiently run on a GPU\@.


\subsection{Attack shader: Union}\label{subsec:attack_shader}

In order to calculate the union of the upwards-closed sets,
we can simply unionize the minimal elements that the sets are represented by:

\[\mathrm{Min} \left( \bigcup_{g' \in \mathrm{Succ}(g)} \uparrow \Updated[g'] \right) =
  \mathrm{Min} \left( \bigcup_{g' \in \mathrm{Succ}(g)}          \Updated[g'] \right)\]

The task thus becomes to simply
collect all energies of the current position's successors,
update them accordingly
and then filter out non-minimal energies.
This workflow is sketched out in Figure~\ref{fig:attack}.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}
    \node[rectangle,draw] (start_node) {\large{$g$}};
    \node (successor1) [right=of start_node,label=above:successors] {$g_1$};

    % Energies of successors
    \node (energy1_1) [right=of successor1] {$e_{g_1,1}$};
    \node (energy1_2) [below] at (energy1_1.south) {$e_{g_1,2}$};
    \node (dots1) [below] at (energy1_2.south) {\rvdots};
    \node (energy2_1) [below] at (dots1.south) {$e_{g_2,1}$};
    \node (energy2_2) [below] at (energy2_1.south) {$e_{g_2,2}$};
    \node (dots2) [below] at (energy2_2.south) {\rvdots};

    \node (successor2) [left=of energy2_1] {$g_2$};
    \node (dots_successors) at (successor2 |- dots2) {\rvdots};

    % Updated energies
    \node (updated1_1) [right=of energy1_1] {$e_{g_1,1}'$};
    \node (updated1_2) [right=of energy1_2] {$e_{g_1,2}'$};
    \node (udots1) at (updated1_2 |- dots1) {\rvdots};
    \node (updated2_1) [right=of energy2_1] {$e_{g_2,1}'$};
    \node (updated2_2) [right=of energy2_2] {$e_{g_2,2}'$};
    \node (udots2) at (updated2_2 |- dots2) {\rvdots};
    % Include energies of g itself
    %\node (energyg_1) [below] at (udots2.south) {$e_{g,1}$};
    %\node (energyg_2) [below] at (energyg_1.south) {$e_{g,2}$};
    %\node (udots3) [below] at (energyg_2.south) {\rvdots};

    % Minimized energies
    \node (minimal1) [right=14mm of udots1.north] {$e_1^*$};
    \node (minimal2) [below] at (minimal1.south) {$e_2^*$};
    \node (mdots) [below] at (minimal2.south) {\rvdots};

    \draw[->] (start_node.east) -- (successor1.west);
    \draw[->] (start_node.east) -- (successor2.west);
    \draw[->] (successor1) -- (energy1_1.west);
    \draw[->] (successor1) -- (energy1_2.west);
    \draw[->] (successor1) -- (energy1_2.west |- dots1);
    \draw[->] (successor2) -- (energy2_1.west);
    \draw[->] (successor2) -- (energy2_2.west);
    \draw[->] (successor2) -- (energy2_2.west |- dots2);

    \draw[->] (energy1_1) -- node (l_update) [above=2mm] {update} (updated1_1);
    \draw[->] (energy1_2) -- (updated1_2);
    \draw[->] (energy1_2.east |- dots1) -- (updated1_2.west |- udots1);
    \draw[->] (energy2_1) -- (updated2_1);
    \draw[->] (energy2_2) -- (updated2_2);
    \draw[->] (energy2_2.east |- dots2) -- (updated2_2.west |- udots2);
    % Energies from g
    %\draw[->,shorten >= 4mm] (start_node.south) |-
    %    node [near end,below] {energies of $g$}
    %    (energyg_2.west);

    % Diagonal lines suggesting the reduction to minimal energies
    \draw[thick] (updated1_1.east |- udots2.south) -- (minimal1.west |- mdots.south);
    \draw[thick] (updated1_1.north east) -- (minimal1.north west);
    \node (l_minimize) [right=7mm of l_update] {minimize};
\end{tikzpicture}
\end{center}
\caption{Data flow for processing Attack positions}%
\label{fig:attack}
\end{figure}

The layout of this figure suggests a very natural way to further parallelize
the processing of attack nodes:
we spawn one thread for each energy tuple.
The shader then has two tasks:
\begin{enumerate}
    \item Update its energy using the correct edge weight,
    \item Figure out if it should be kept as a minimal energy or discarded.
\end{enumerate}

The update step basically follows the definition of $\mathsf{upd}^{-1}$.
Minimizing the energies is done by comparing all energy tuples with each other,
requiring a quadratic runtime:
Let $n = \sum_{g' \in \mathrm{Succ}(g)} | \Energies[g'] |$ be the total number
of energies considered for a starting node $g$.
We do $n^2$ comparisons in total, where each thread checks all $n$
energies to find out if this thread's energy is part of the minimal set.
If the thread for energy $e$ encounters an energy $e_2$ with $e_2 \leq e$,
energy $e$ is not minimal and will be filtered out. If $e = e_2$, the thread
index is used as a tie breaker to avoid any duplicates.
The shader code performing this minimization step is shown in
Listing~\ref{lst:minimize}.

\begin{lstlisting}[language=WGSL,float,
    caption={WGSL Shader Code for minimizing energies.
        \texttt{i} is the thread ID
        and determines the energy that this thread processes.
        It is compared against energies at index \texttt{j},
        which loops through the range of energies to minimize,
        denoted by \texttt{e\_start} and \texttt{e\_end}.
        The function \texttt{less\_eq} is shown in Listing~\ref{lst:less_eq}.},
    label=lst:minimize]
let energy = energies[i]; // `i' is the thread ID
var filter_out = 0;
for (var j = e_start; j < e_end; j++) {
    let e2 = energies[j];
    // Skip reflexive comparisons.
    // When energies are equal,
    // keep only those with higher index
    let eq = e2 == energy;
    if j != i && ((eq && i < j) ||
                  (!eq && less_eq(e2, energy))) {
        // Mark to be filtered out
        filter_out = 1;
        break;
    }
}
\end{lstlisting}

Theoretically only half of the comparisons could be performed by pruning symmetric
pairs (i.e.\ compare only $e_1$ with $e_2$, not also $e_2$ with $e_1$),
but this would make the comparison itself more expensive by not only requiring
the componentwise comparison of $e_1 \leq e_2$, but also $e_1 \geq e_2$.
Even bigger problems arise when trying to map this approach to the GPU
execution model:
A thread would possibly have to write flags for which energies to keep not only
for its \enquote{own} energy,
but for any energy it's comparing it with.
Such cross-thread memory writes make synchronization more difficult.
Distributing symmetry-reduced comparisons to multiple threads is also
more complex than each thread just comparing its energy with all others.


\subsection{Defend shader: Intersection}\label{subsec:defend_shader}

Computing the intersection of upwards-closed sets for processing defend nodes
is significantly more complex than what was done for attack nodes.
The idea behind taking the intersection is that we want to find the set of
energies with which the attacker can win,
regardless of the choice the defender makes.

If $E_1, E_2 \subseteq \mathbf{En}$ are two sets of minimal energies
representing the upwards-closed sets $\uparrow E_1$ and $\uparrow E_2$,
their intersection can be obtained by taking the supremum of all their combinations:
\begin{equation*}
    \mathrm{Min} (\uparrow E_1 \cap \uparrow E_2 ) =
    \mathrm{Min} (\{ \sup(e_1, e_2) \mid e_1 \in E_1,\ e_2 \in E_2 \})
\end{equation*}

In order to handle multiple sets, we iteratively apply that step,
minimizing the result each time.
This approach was proposed in~\cite{brihaye2023multi}.
Line 14 of Algorithm~\ref{alg:energy_game} thus becomes more explicitly:

\begin{algorithm}[H]\label{alg:intersection}
    \DontPrintSemicolon
    \SetKwData{Intersection}{intersection}
    \SetKwData{New}{new\_energies}

    $\New = \Updated[g_1]$ for some $g_1 \in \mathrm{Succ}(g)$\;
    \For{$g' \in \mathrm{Succ}(g) \setminus \{g_1\}$}{
        $\New = \{\sup(e_1, e_2) \mid e_1 \in \New,\ e_2 \in \Updated[g']\}$\;
        $\New = \mathrm{Min} (\New)$\;
    }

    \caption{Intersection of Upwards-Closed Sets}
\end{algorithm}

% Direct approach
The algorithm presented in~\cite{bisping2023process} does this a bit
differently by intersecting all sets at once.
This requires producing combinations over all sets,
whose number for a defense node $g \in G_d$ is given as
$\prod_{g' \in \mathrm{Succ}(g)} |\Energies[g']|$,
which is exponential in the out-degree of $g$.
For example, if a defense node had 10 successors with 5 energy tuples each,
we would have to consider $5^{10} = 9\,765\,625$ combinations.
In such cases the vast majority of these combinations will be superfluous and 
the number of energy tuples left after minimization is usually in the single
digits.

The exact number of combinations produced in the iterative approach shown in
Algorithm~\ref{alg:intersection} cannot easily be predicted as it depends on
the size of the energies after each minimization step,
but it should never be significantly higher than the final result.
Ultimately, this gets rid of the out-degree as an exponential factor in the
run-time complexity~\cite{brihaye2023multi},
which helps in ensuring the scalability of the implementation.

% Difficult to predict final and intermediate size: 64 threads/position
On the other hand, the fact that the number of combinations cannot be predicted
without essentially running through the entire algorithm,
also makes it difficult to accurately dispatch the required amount of GPU
threads.
So instead of having one thread per combination,
which would ideally distribute the workload,
each position is statically allocated one workgroup of 64 threads.
This means on smaller positions many threads may sit idle,
while on larger positions the workgroup will have to perform the same
operations multiple times in a loop to cover all combinations.

However one big advantage is also achieved by processing a defense position in
exactly one workgroup:
the ability for precise synchronization.
As touched on in Section~\ref{sec:gpu_model},
while inter-workgroup synchronization is not supported by \mbox{WebGPU},
synchronization within a workgroup can easily be done.
Synchronization is even required in order to properly implement
Algorithm~\ref{alg:intersection}, in particular for the minimization step.

\subsubsection{Compacting energies}

While the attack shader~(\ref{subsec:attack_shader}) avoids the need for
synchronization by only flagging each energy on whether or not it is minimal,
in this case we need to continue working with the minimal set in the next
iteration.
To that end it is necessary to compact the energy list by removing non-minimal
entries and shifting all others to the left to fill any gaps.
First, a list of flags for each combination is produced, like in the attack
shader:
If $[c_1, c_2, \ldots, c_n]$ is the list of combinations,
we create the flags $[f_1, \ldots, f_n]$ where $f_i = 1$ if $c_i$ is not minimal
and should be removed, otherwise $f_i = 0$.
Next we calculate the prefix sums $[p_1, \ldots, p_n]$ of the flag list,
which is for each element the sum of all preceding elements:
$p_i = \sum_{j = 0}^{i - 1} f_j$.

Due to its usefulness, the prefix sum algorithm has been widely studied in
relation to GPU-programming.
The implementation used is based on~\cite{Harris2011ParallelPS}.
Again, while it can be done fairly easily within a workgroup,
prefix sum for larger arrays across workgroups would require global
synchronization.

These prefix sums now represent the number of non-minimal gaps to the left of
each combination.
Each minimal combination $c_i$ can now be shifted left by $p_i$ steps to index
$i - p_i$.


\section{Data Layout}

An efficient implementation of the processes described above requires some
thought to be put into the data layout,
especially with the restrictive nature of the GPU architecture.
The highly parallelized execution model makes dynamic memory allocation very
unsuitable.
Instead, all required memory must be allocated before running a shader.

However in the case of the defend shader,
the exact amount of combinations that will be produced
and thus the required memory size cannot be predicted in advance.
To cope with that uncertainty,
each position is initially allocated memory for 64 energy tuples.
If while executing the shader that turns out to be insufficient,
the position is processed again later with double the amount memory until it
succeeds.
In practice however, 64 energy tuples is almost always enough.

Some of the input data for the shaders, such as the game graph,
remains static during the runtime of the algorithm
and only has to be sent to the GPU once.
Other data must be newly copied into GPU memory before every iteration of
shader calls.
This includes an array containing all the energies of the successors of
positions currently in the visit list.
Additional data structures tell where the entries for each position from the
visit list begin,
and which energies belong to what successor.
The following two sections detail some of the techniques used to efficiently
store the data in memory.


\subsection{CSR Graph format}

One major structure is the game graph,
which is required in the shaders for retrieving the energy updates.
It is a directed graph with an energy update associated to each edge.
One option for representing that graph would be an adjacency matrix,
where for any two positions it is saved if there is an edge between them
along with the energy update for that edge.
That matrix would require $|G|^2$ entries, and since the game graph is
relatively sparse, the majority of those entries would be empty.
Such a high memory overhead would not make for a scalable implementation,
especially since the game graphs can easily get very large.
A common way of more efficiently storing graphs or sparse matrices in general is
the \emph{compressed sparse row} format (CSR)~\cite{Merrill2015,Hijma2023}.

The CSR format consists of three flat arrays.
The array $C$ holds all \emph{column-indices} of non-empty matrix cells in order.
It can be thought of as the concatenation of all adjacency lists for our graph.
Another array $R$ holds the \emph{row-offsets}, meaning the positions in array
$C$ where each row of the matrix starts.
Additional data for each edge can be stored in another array $W$, which has the
same length as $C$.
We use this third array for storing the energy updates.
Figure~\ref{fig:csr} illustrates this format on a small example graph.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}[position/.style={circle,draw}]
    \node[position] (0) {0};
    \node[position] (1) [right=of 0] {1}
        edge [<-] node[above] {$u_0$} (0);
    \node[position] (2) [below=of 0] {2}
        edge [<-] node[auto] {$u_1$} (0)
        edge [<-] node[near end,above] {$u_3$} (1);
    \node[position] (3) [below=of 1] {3}
        edge [<-] node[near end,above] {$u_2$} (0)
        edge [->] node[right] (u4) {$u_4$} (1)
        edge [->] node[auto] {$u_5$} (2);
    \node[right=6mm of u4] (matrix) {
        $\begin{bmatrix}
            - & u_0 & u_1 & u_2 \\
            - & -   & u_3 & -   \\
            - & -   & -   & -   \\
            - & u_4 & u_5 & -
        \end{bmatrix}$
    };
    \node[right=6mm of matrix] {
        $\begin{aligned}
            C &= [1, 2, 3, 2, 1, 2]\\
            R &= [0, 3, 4, 4, 6]\\
            W &= [u_0, u_1, u_2, u_3, u_4, u_5]
        \end{aligned}$
    };
\end{tikzpicture}
\end{center}
\caption{Example graph with adjacency matrix and CSR representation.
    $u_0, \ldots, u_5 \in \mathbf{Up}$ are the energy updates.
}%
\label{fig:csr}
\end{figure}

The CSR-format is very efficient in terms of memory usage and access,
but its main drawback is that inserting or deleting edges and nodes is very
expensive, since it requires shifting large parts of the arrays.
This is not a problem in our case, since we generate the game graph only once
in the beginning and then never change it afterwards.
Even though this format was chosen primarily with regards to the needs
of the GPU code, it also turned out to be very valuable to store the graph in
the same format in the host memory of the CPU\@.


\subsection{Storing Energy Sets}\label{subsec:energy_memory}

While running the algorithm, we need to keep track of a set of energy tuples
for each position in the game graph.
This corresponds to the \Energies array of Algorithm~\ref{alg:energy_game}.
The algorithm ensures that only minimized sets are stored,
meaning we don't use up more memory than necessary to represent the
upwards-closed sets.
An alternative representation of upwards-closed sets has been proposed
in~\cite{Delzanno2000}, where the minimal tuples are generated by the possible
paths in a specially constructed graph.
While that representation might allow for more optimized memory usage in some cases,
it didn't seem worth the additional complexity of having to handle a graph structure.
Particularly with regards to GPU-programming, a flat list of tuples is much
easier to work with.

In the case of this spectroscopy algorithm, we can significantly reduce the
size of the energy sets by constraining the range of the energy values.
Ultimately, we want to compare the winning budgets with the energy levels
corresponding to the different notions of equivalence.
However these energy levels don't require any distinction of values above 2:
we can consider any values of 3 or above to be infinite without losing
information about the equivalences that we care about.
We can thus restrict the values of the energy tuples to $\{0, 1, 2, \infty\}$,
with updates saturating at $\infty$.

For once, this gives an upper bound to the size of antichains that might
possibly need to be stored as minimal sets.
Secondly, the 4 possible values can be stored in just 2 bits of memory
($\infty$ is represented by 3, or binary $11_2$).
The 6-tuples required for the spectroscopy algorithm can thus be bit-packed
into 12 bits.
The shader language WGSL includes only very few data types compared to most CPU
programming languages.
The only integer data types are the signed (\texttt{i32}) and unsigned
(\texttt{u32}) 32-bit integers,
so each energy tuple is stored in a \texttt{u32},
which could hold tuples with up to 16 values that are capped at 3.
Listing~\ref{lst:less_eq} shows how these bit-packed energies can
be worked with in shader code to test the componentwise $\leq$-relation.
Equality between two energies can be directly tested by comparing the two
\texttt{u32}s,
as is done in line~8 of Listing~\ref{lst:minimize}.

\begin{lstlisting}[language=WGSL,float,
    caption={WGSL Shader Code handling bit-packed energies
             to calculate if $a \leq b$.
             The bitwise AND operation \texttt{a~\&~0x3} sets all but the 2
             least significant bits to 0,
             thus comparing just the first field.
             This is repeated with all six fields,
             where each bit mask is written in hexadecimal format.
             For example, hexadecimal C0$_{16}$ is $\ldots0\,1100\,0000_2$ in
             binary, selecting the fourth 2-bit field.
             },
    label=lst:less_eq]
fn less_eq(a: u32, b: u32) -> bool {
    return (
        (a & 0x3)   <= (b & 0x3)   &&
        (a & 0xc)   <= (b & 0xc)   &&
        (a & 0x30)  <= (b & 0x30)  &&
        (a & 0xc0)  <= (b & 0xc0)  &&
        (a & 0x300) <= (b & 0x300) &&
        (a & 0xc00) <= (b & 0xc00)
    );
}
\end{lstlisting}

Similarly, update tuples $(u_1, \ldots, u_6) \in \mathbf{Up}_6$
for the spectroscopy algorithm can also be packed into a single \texttt{u32}.
According to Definition~\ref{def:update}, each update value $u_k$ can be either
no update at all (0),
a decrement update (-1),
or a min-update.
At this point we constrain ourselves to min-updates $\mathtt{min}_D$  % chktex 35
where $|D| = 2$.
Since the index of the update value in the tuple, $k$, must always be a member of $D$,
the only information we really need is the index of the other element, $D \setminus \{k\}$.
With $N$-dimensional updates, this comes out to $N + 2$ distinct update values.
In our case of 6 dimensions, we need to represent 8 values,
which can fit in 3 bits.
For better alignment we choose the next power
of 2 and set aside 4 bits for each update field.
The exact binary representation for an update value $u_k$ is shown in
Table~\ref{tab:update_encoding}.

\begin{table}[ht]
\centering
\caption{Binary encoding of updates. $k$ is the index of this update value in
the tuple.}\label{tab:update_encoding}
\begin{tabular}{ r c l }
    \toprule
    Decimal & Binary & Update \\
    \midrule
    0 & $0000_2$ & 0 \emph{(No update)} \\
    1 & $0001_2$ & -1 \\
    2 & $0010_2$ & $\mathtt{min}_{k, 1}$ \\ % chktex 35
    3 & $0011_2$ & $\mathtt{min}_{k, 2}$ \\ % chktex 35
    \vdots & \vdots & \vdots \\
    7 & $0111_2$ & $\mathtt{min}_{k, 6}$ \\ % chktex 35
    \bottomrule
\end{tabular}
\end{table}

%TODO: Does this still need \newpage for better layout?
While this section focused on 6-dimensional energies with maximal values of 3,
the implementation also supports more general energy games with arbitrary
dimensionality and value range by doing shader preprocessing.


\section{Limitations}

The high complexity of the spectroscopy algorithm,
both in runtime and memory usage,
stems from having to handle the energy sets
and also just the sheer size of the game graph,
which is usually significantly larger than the underlying transition system.
The shaders take care of updating and combining energy sets,
which makes up the \enquote{innermost loop} of the algorithm,
in a highly parallelized fashion. 
But a fair amount of work still has to be done
outside of that by the CPU,
which could not be efficiently parallelized for various reasons.

Ideally, most of the data needed during the algorithm would stay on the GPU,
so that less data needs to be copied between GPU and CPU memory.
But currently, the energy sets for each position reside in CPU memory.
Only the energies needed in each iteration are sent over to the GPU,
and the resulting energy sets are then read back and saved.
Keeping the energy sets in GPU memory instead would require some sort of
dynamic memory allocation,
since the size of the energy sets can grow almost arbitrarily
and the order in which energy sets are changed depends on the structure of the
graph.
There exist some approaches to handle dynamic data like that on a GPU,
for example the \emph{Hornet} data structure by \textcite{Busato2018}.
But its high design complexity puts that out of scope for this work.

Due to the size of the game graph, the process of generating it based on an
input LTS is also a tempting point for optimization.
But for similar reasons, this process had to be fully done on the CPU\@.
The generated graph is again a highly irregular data structure and
even parallelizing only the most critical parts couldn't achieve a speedup.
The fact that the positions themselves contain arbitrarily-sized sets of
processes doesn't help either.
In a test of only generating a large number defense positions,
the shader took noticeably more time than a pure CPU-implementation of the same
process.

\subsection{Hardware limits}

Even though modern graphics cards can achieve truly incredible degrees of
parallelization,
there are still some hardware limits that we need to consider.
We cannot spawn arbitrarily many threads at once.
Most current GPUs have a limit of 65535 workgroups.
With 64 threads per workgroup that amounts to about 4.2 million threads,
which is the limit of what can be scheduled at once, the driver will most
likely not execute all threads simultaneously.

That means that on very large inputs,
we may not be able to process the entire visit list in a single shader call.
Instead, only part of the visit may be processed at once,
taking into account the thread limit as well as memory usage.
The algorithm is still correct,
even if only some subset of the visit list is processed.
That is because the outcome of processing a position is only dependent
on the energies of its successors.
Whenever any of its successors gets changed,
the position is again added to the visit list.
Processing positions in a different order may therefore cause some of them
to be processed more often,
but the final result will not be affected.

Memory availability is another limiting factor.
The larger transition systems from the VLTS benchmarking suite~\cite{vlts}
caused the game graph generation to run out of memory
on a system with 32GB RAM\@.
But even if CPU and GPU both had sufficient memory,
in many situations the size of a single GPU buffer allocation
is limited to $2^{31} - 1$ bytes or 2GiB,
the maximum range of a signed 32-bit integer.
The largest allocation is required for the game graph,
where the energy updates require 4 bytes per edge.
This limits the size of the game graph to
$2^{29} - 1 = 536\,870\,911$ edges.
Based on the benchmarks shown in Chapter~\ref{ch:benchmarks},
processing such a maximally-sized graph would probably take around 4 minutes.
