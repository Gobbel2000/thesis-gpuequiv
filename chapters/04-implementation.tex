At the center of this contribution is a parallel GPU implementation
of the energy game introduced in Section~\ref{sec:energy_games}.
Given a game graph as input, the program calculates the energy budgets
required for the attacker to win the game.
This algorithm was described by \textcite{bisping2023process} as part of the
spectroscopy algorithm, as well as by \textcite{brihaye2023multi},
which focuses on \enquote{Multi-Weighted Reachability Games} in general,
but presents essentially the same algorithm structure.

\section{Parallelizing the Algorithm}\label{sec:parallelization}

\newcommand{\GPUline}{%
    \SetNlSty{textbf}{\colorbox{black!30!red!15}}{}%
}

% First, definitions that are also used outside the algorithm
\SetKwData{Updated}{updated}
\SetKwData{Energies}{energies}
\begin{algorithm}[ht]
    \DontPrintSemicolon
    \SetKwData{Start}{start}
    \SetKwData{Visit}{visit}
    \SetKwData{New}{new\_energies}
    \SetKwFor{pFor}{for parallel}{do}{end}

    $\Start = \{g \in G_d \mid \mathrm{Succ}(g) = \emptyset\}$\;\nllabel{ln:start}
    \lFor{$g \in \Start$}{$\Energies[g] = \{\mathbf{0}\}$}
    \lFor{$g \in G \setminus \Start$}{$\Energies[g] = \emptyset$}\nllabel{ln:empty}
    $\Visit = \bigcup_{g \in \Start} \mathrm{Pred}(g)$\;

    \BlankLine
    \While{$\Visit \neq \emptyset$}{
        \pFor{$g \in \Visit$}{
            $\Visit = \Visit \setminus \{g\}$\;
            \pFor{$g' \in \mathrm{Succ}(g)$} {
                \GPUline
                $\Updated[g'] = \{ \mathsf{upd}^{-1}(e, w(g, g')) \mid e \in \Energies[g'] \}$
            }

            \BlankLine
            \eIf(\tcp*[h]{attack positions}){$g \in G_a$\nllabel{ln:new_start}}{
                \GPUline
                $\New = \mathrm{Min} \left(
                    \bigcup_{g' \in \mathrm{Succ}(g)} \upclosed \Updated[g']
                \right)$
            } (\tcp*[h]{defend positions}) {
                \GPUline
                $\New = \mathrm{Min} \left(
                    \bigcap_{g' \in \mathrm{Succ}(g)} \upclosed \Updated[g']
                \right)$\nllabel{ln:intersection}
            }\nllabel{ln:new_end}

            \BlankLine
            \If{$\New \neq \Energies[g]$}{
                $\Energies[g] = \New$\;
                $\Visit = \Visit \cup \mathrm{Pred}(g)$\;
            }
        }
    }
    \Return{\Energies}

    \caption{Parallel Energy Game. Highlighted lines are executed by the GPU\@.}%
    \label{alg:energy_game}
\end{algorithm}

The basic structure of the parallel implementation
is as shown in Algorithm~\ref{alg:energy_game}.
In order to calculate all the winning budgets,
we start at the end of the game graph and work our way backwards.
The attacker immediately wins at all defender positions with no successors,
so there we can set the energies required for the attacker to win to the
0-valued energy tuple (lines~\ref{ln:start}--\ref{ln:empty}).
In every iteration,
a set of positions is visited and its associated energies are updated.
The next iteration visits the predecessors of all positions whose energies
have changed in the previous iteration,
thus walking backwards in the game graph in a breadth-first manner.
Once an iteration causes no changes to the energies, the algorithm terminates.

Walking backwards requires using the inverse update function
$\mathsf{upd}^{-1}$.
Calling it on an energy $e$ and an update $u$, $\mathsf{upd}^{-1}(e, u) = e'$,
produces the least energy tuple $e'$, such that $\mathsf{upd}(e', u) \geq e$.
Decrements ($-1$) are simply subtracted instead of added,
but for $\minupd$-updates, a maximum needs to be created instead:
For an update element $\minupd_{\{k, i\}}$ at position $u_k$,
we need to set $e'_i = \max(e_k, e_i)$,
so that we correctly have $e_k = \min(e'_k, e'_i)$ in the other direction.
For example,
$\mathsf{upd}^{-1}((1, 0), (\minupd_{\{1, 2\}}, 0)) = (1, 1)$
is the lowest energy tuple that satisfies
$\mathsf{upd}((1, 1), (\minupd_{\{1, 2\}}, 0)) = (1, 1)
    \geq (1, 0)$

We achieve parallelization by visiting all positions in the visit list
in parallel.
But to fully take advantage of the massively multithreaded
GPU architecture, each position is further processed by multiple threads.

The core of the algorithm lies in lines~\ref{ln:new_start}--\ref{ln:new_end},
where the upward-closed sets of energies are either joined or intersected.
Since both operations demand very different approaches,
handling them in a single shader would inevitably incur high branch-divergence
(see Section~\ref{sec:gpu_model}).
Therefore both cases are handled by separate, more regular shaders:
one for processing attack positions, another for defend positions.
This technique is known as \enquote{Kernel fission}
and improves resource utilization by allowing threads in a work group to follow
more similar instruction paths~\cite{Hijma2023}.
The following two sections detail how these operations have been implemented to
efficiently run on a GPU\@.


\subsection{Attack Shader: Union}\label{subsec:attack_shader}

In order to get the union of the upward-closed sets,
we can simply take the union of the minimal elements
that the sets are represented by:

\[\mathrm{Min} \left( \bigcup_{g' \in \mathrm{Succ}(g)} \upclosed \Updated[g'] \right) =
  \mathrm{Min} \left( \bigcup_{g' \in \mathrm{Succ}(g)}           \Updated[g'] \right)\]

The task thus becomes to simply
collect all energies of the current position's successors,
update them accordingly
and then filter out non-minimal energies.
This workflow is sketched out in Figure~\ref{fig:attack}.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}
    \node[rectangle,draw] (start_node) {\large{$g$}};
    \node (successor1) [right=of start_node] {$g_1$};
    \node (l_successors) [above=2.7mm] at (successor1) {successors};

    % Energies of successors
    \node (energy1_1) [right=of successor1] {$e_{g_1,1}$};
    \node (energy1_2) [below] at (energy1_1.south) {$e_{g_1,2}$};
    \node (dots1) [below] at (energy1_2.south) {\rvdots};
    \node (energy2_1) [below] at (dots1.south) {$e_{g_2,1}$};
    \node (energy2_2) [below] at (energy2_1.south) {$e_{g_2,2}$};
    \node (dots2) [below] at (energy2_2.south) {\rvdots};

    \node (successor2) [left=of energy2_1] {$g_2$};
    \node (dots_successors) at (successor2 |- dots2) {\rvdots};

    % Updated energies
    \node (updated1_1) [right=of energy1_1] {$e_{g_1,1}'$};
    \node (updated1_2) [right=of energy1_2] {$e_{g_1,2}'$};
    \node (udots1) at (updated1_2 |- dots1) {\rvdots};
    \node (updated2_1) [right=of energy2_1] {$e_{g_2,1}'$};
    \node (updated2_2) [right=of energy2_2] {$e_{g_2,2}'$};
    \node (udots2) at (updated2_2 |- dots2) {\rvdots};

    % Minimized energies
    \node (minimal1) [right=14mm of udots1.north] {$e_1^*$};
    \node (minimal2) [below] at (minimal1.south) {$e_2^*$};
    \node (mdots) [below] at (minimal2.south) {\rvdots};

    \draw[->] (start_node.east) -- (successor1.west);
    \draw[->] (start_node.east) -- (successor2.west);
    \draw[->] (successor1) -- (energy1_1.west);
    \draw[->] (successor1) -- (energy1_2.west);
    \draw[->] (successor1) -- (energy1_2.west |- dots1);
    \draw[->] (successor2) -- (energy2_1.west);
    \draw[->] (successor2) -- (energy2_2.west);
    \draw[->] (successor2) -- (energy2_2.west |- dots2);

    \draw[->] (energy1_1) -- node (l_update) [above=2mm] {update} (updated1_1);
    \draw[->] (energy1_2) -- (updated1_2);
    \draw[->] (energy1_2.east |- dots1) -- (updated1_2.west |- udots1);
    \draw[->] (energy2_1) -- (updated2_1);
    \draw[->] (energy2_2) -- (updated2_2);
    \draw[->] (energy2_2.east |- dots2) -- (updated2_2.west |- udots2);

    % Diagonal lines suggesting the reduction to minimal energies
    \draw[thick] (updated1_1.east |- udots2.south) -- (minimal1.west |- mdots.south);
    \draw[thick] (updated1_1.north east) -- (minimal1.north west);
    \node (l_minimize) [base right=7mm of l_update] {minimize};

    % CPU/GPU labels
    \node (cpu) at (start_node |- dots_successors.south) {CPU};
    \node (gpu) at (minimal1 |- cpu) {GPU};

    % Background
    \begin{scope}[on background layer]
        \fill[black!60!blue!7]
            (cpu.west |- l_minimize.north) rectangle
            ([xshift=-1mm] energy2_2.east |- cpu.south);
        \fill[black!30!red!15]
            ([xshift=-1mm] energy2_2.east |- cpu.south) rectangle
            (gpu.east |- l_minimize.north);
    \end{scope}
\end{tikzpicture}
\end{center}
\caption{Data flow for processing an attacker position $g$.
    Task distribution between CPU and GPU is also marked.
}%
\label{fig:attack}
\end{figure}

The layout of this figure suggests a very natural way to further parallelize
the processing of attack nodes:
we spawn one thread for each energy tuple.
The shader then has two tasks:
\begin{enumerate}
    \item Update its energy using the correct edge weight,
    \item Figure out if it should be kept as a minimal energy or discarded.
\end{enumerate}

The update step basically follows the definition of $\mathsf{upd}^{-1}$.
Minimizing the energies is done by comparing all energy tuples with each other,
requiring a quadratic runtime:
Let $n = \sum_{g' \in \mathrm{Succ}(g)} | \Energies[g'] |$ be the total number
of energies considered for a starting node $g$.
We do $n^2$ comparisons in total, where each thread checks all $n$
energies to find out if this thread's energy is part of the minimal set.
If the thread for energy $e$ encounters an energy $e_2$ with $e_2 \leq e$,
energy $e$ is not minimal and will be filtered out. If $e = e_2$, the thread
index is used as a tie breaker to avoid any duplicates.
The shader code performing this minimization step is shown in
Listing~\ref{lst:minimize}.

\begin{lstlisting}[language=WGSL,float,
    caption={WGSL Shader Code for minimizing energies.
        \texttt{i} is the thread ID
        and determines the energy that this thread processes.
        It is compared against energies at index \texttt{j},
        which loops through the range of energies to minimize,
        denoted by \texttt{e\_start} and \texttt{e\_end}.
        The function \texttt{less\_eq} is shown in Listing~\ref{lst:less_eq}.},
    label=lst:minimize]
let energy = energies[i]; // `i' is the thread ID
var is_minimal = 1;
for (var j = e_start; j < e_end; j++) {
    let e2 = energies[j];
    // Skip reflexive comparisons. When energies are equal,
    // keep only those with lower index.
    let eq = e2 == energy;
    if j != i && ((eq && i > j) ||
                  (!eq && less_eq(e2, energy))) {
        // Mark to be filtered out
        is_minimal = 0;
    }
}
\end{lstlisting}

Theoretically, only half of the comparisons could be performed
by pruning symmetric pairs
(i.e.\ compare only $e_1$ with $e_2$, not also $e_2$ with $e_1$),
but this would make the comparison itself more expensive by not only requiring
the componentwise comparison of $e_1 \leq e_2$, but also $e_1 \geq e_2$.
Even bigger problems arise when trying to map this approach to the GPU
execution model:
A thread would possibly have to write flags for which energies to keep not only
for its \enquote{own} energy,
but for any energy it's comparing it with.
Such cross-thread memory writes make synchronization more difficult.
Distributing symmetry-reduced comparisons to multiple threads is also
more complex than each thread just comparing its energy with all others.


\subsection{Defend Shader: Intersection}\label{subsec:defend_shader}

Computing the intersection of upward-closed sets for processing defend nodes
is significantly more complex than what was done for attack nodes.
The idea behind taking the intersection is that we want to find the set of
energies with which the attacker can win,
regardless of the choice the defender makes.

If $E_1, E_2 \subseteq \mathbf{En}$ are two sets of minimal energies
representing the upward-closed sets $\upclosed E_1$ and $\upclosed E_2$,
their intersection can be obtained
by taking the suprema of all their combinations:
\begin{equation*}
    \mathrm{Min} (\upclosed E_1 \cap \upclosed E_2 ) =
    \mathrm{Min} (\{ \sup(e_1, e_2) \mid e_1 \in E_1,\ e_2 \in E_2 \})
\end{equation*}

In order to handle multiple sets, we iteratively apply this step,
minimizing the result each time.
This approach was proposed by \textcite{brihaye2023multi}.
Line~\ref{ln:intersection} of Algorithm~\ref{alg:energy_game}
thus becomes more explicitly:

\begin{algorithm}[H]\label{alg:intersection}
    \DontPrintSemicolon
    \SetKwData{Intersection}{intersection}
    \SetKwData{New}{new\_energies}

    $\New = \Updated[g_1]$ for some $g_1 \in \mathrm{Succ}(g)$\;
    \For{$g' \in \mathrm{Succ}(g) \setminus \{g_1\}$}{
        $\New = \{\sup(e_1, e_2) \mid e_1 \in \New,\ e_2 \in \Updated[g']\}$\;
        $\New = \mathrm{Min} (\New)$\;
    }

    \caption{Intersection of Upward-Closed Sets}
\end{algorithm}

% Direct approach
The algorithm presented by \textcite{bisping2023process} does this a bit
differently by intersecting all sets at once.
This requires producing combinations over all sets,
whose number for a defense node $g \in G_d$ is given as
$\prod_{g' \in \mathrm{Succ}(g)} |\Energies[g']|$,
which is exponential in the out-degree of $g$.
For example, if a defense node had 10 successors with 5 energy tuples each,
we would have to consider $5^{10} = 9\,765\,625$ combinations.
In such cases the vast majority of combinations will be superfluous and
the number of energy tuples left after minimization is usually in the single
digits.

The exact number of combinations produced in the iterative approach shown in
Algorithm~\ref{alg:intersection} cannot easily be predicted,
as it depends on the size of the energies after each minimization step,
but it should never be significantly higher than the final result.
Ultimately, this gets rid of the out-degree as an exponential factor in the
run-time complexity~\cite{brihaye2023multi},
which helps in ensuring the scalability of the implementation.

% Difficult to predict final and intermediate size: 64 threads/position
On the other hand, the fact that the number of combinations cannot be predicted
without essentially running through the entire algorithm
also makes it difficult to accurately dispatch the required amount of GPU
threads.
So instead of having one thread per combination,
which would ideally distribute the workload like in the attack shader,
each position is statically allocated one workgroup of 64 threads here.
This means, on smaller positions many threads may sit idle,
while on larger positions the workgroup will have to perform the same
operations multiple times in a loop to cover all combinations.

However, one big advantage is also achieved by processing a defense position in
exactly one workgroup:
the ability for precise synchronization.
As touched on in Section~\ref{sec:gpu_model},
while inter-workgroup synchronization is not supported by \mbox{WebGPU},
synchronization within a workgroup can easily be done.
Synchronization is even required in order to properly implement
Algorithm~\ref{alg:intersection}, in particular for the minimization step.

\subsubsection{Compacting Energies}

While the attack shader~(\ref{subsec:attack_shader}) avoids the need for
synchronization by only flagging each energy on whether or not it is minimal,
in this case we need to continue working with the minimal set in the next
iteration.
This requires compacting the energy list by removing non-minimal
entries and shifting all others to the left to fill any gaps.

For this step, a parallel prefix sum is computed.
Taking the prefix sum of an array of numbers $[a_1, \ldots, a_n]$
results in an array $[p_1, \ldots, p_n]$ where each entry is the
sum of all preceding elements:
$p_i = \sum_{j = 0}^{i - 1} a_j$.
Due to its usefulness, the prefix sum algorithm has been widely studied in
relation to GPU programming.
The implementation used is based on~\cite{Harris2011ParallelPS}.
Again, while it can be done fairly easily within a workgroup,
prefix sum for larger arrays across workgroups would require global
synchronization.

The way we use the prefix sum for compacting energy values is sketched out
in Figure~\ref{fig:compacting}.
First, a list of flags for each energy is produced, like in the attack shader:
If an energy tuple is minimal, the flag is set to 1, otherwise to 0.
By calculating the prefix sum of this array of flags,
we end up with the number of minimal energies to the left of each element,
which can be used as the new index to which all minimal energies are moved.

\begin{figure}[htpb]
\centering
\begin{tikzpicture}[scale=0.6,thick]
    % Energies
    \draw (0, 0) node {$2, 2, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (2, 0) node {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (4, 0) node {$1, 0, 2$} +(-1, -.5) rectangle ++(1, .5);
    \draw (6, 0) node {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (8, 0) node {$1, 2, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (10, 0) node {$2, 2, 2$} +(-1, -.5) rectangle ++(1, .5);

    % Minimal flags
    \draw (0, -1.5) node[gray] {0} +(-1, -.5) rectangle ++(1, .5);
    \draw (2, -1.5) node {1} +(-1, -.5) rectangle ++(1, .5);
    \draw (4, -1.5) node {1} +(-1, -.5) rectangle ++(1, .5);
    \draw (6, -1.5) node[gray] {0} +(-1, -.5) rectangle ++(1, .5);
    \draw (8, -1.5) node {1} +(-1, -.5) rectangle ++(1, .5);
    \draw (10, -1.5) node[gray] {0} +(-1, -.5) rectangle ++(1, .5);

    % Prefix sum
    \draw (0, -3) node[gray] {0} +(-1, -.5) rectangle ++(1, .5);
    \draw (2, -3) node {0} +(-1, -.5) rectangle ++(1, .5);
    \draw (4, -3) node {1} +(-1, -.5) rectangle ++(1, .5);
    \draw (6, -3) node[gray] {2} +(-1, -.5) rectangle ++(1, .5);
    \draw (8, -3) node {2} +(-1, -.5) rectangle ++(1, .5);
    \draw (10, -3) node[gray] {3} +(-1, -.5) rectangle ++(1, .5);

    % Energies again
    \draw (0, -4.5) node[gray] {$2, 2, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (2, -4.5) node {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (4, -4.5) node {$1, 0, 2$} +(-1, -.5) rectangle ++(1, .5);
    \draw (6, -4.5) node[gray] {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (8, -4.5) node {$1, 2, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (10, -4.5) node[gray] {$2, 2, 2$} +(-1, -.5) rectangle ++(1, .5);

    % Minimal energies
    \draw[gray] (6, -6.5) node {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw[gray] (8, -6.5) node {$1, 2, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw[gray] (10, -6.5) node {$2, 2, 2$} +(-1, -.5) rectangle ++(1, .5);
    \draw (0, -6.5) node {$2, 1, 1$} +(-1, -.5) rectangle ++(1, .5);
    \draw (2, -6.5) node {$1, 0, 2$} +(-1, -.5) rectangle ++(1, .5);
    \draw (4, -6.5) node {$1, 2, 1$} +(-1, -.5) rectangle ++(1, .5);

    % Diagonal arrows
    \draw[->] (2, -5) -- (0, -6);
    \draw[->] (4, -5) -- (2, -6);
    \draw[->] (8, -5) -- (4, -6);

    % Vertical arrows
    \foreach \x in {0,2,...,10}
    {
        \draw[->] (\x, -0.5) -- (\x, -1);
        \draw[->] (\x, -2) -- (\x, -2.5);
        %\draw[->] (\x, -3.5) -- (\x, -4);
    }

    % Background highlight of minimal energies
    \begin{scope}[on background layer]
        \fill[blue!20] (1, -1) rectangle (5, -5);
        \fill[blue!20] (5, -5) -- (3, -6) -- (-1, -6) -- (1, -5);
        \fill[blue!20] (7, -1) rectangle (9, -5);
        \fill[blue!20] (9, -5) -- (5, -6) -- (3, -6) -- (7, -5);
        \fill[blue!20] (-1, -6) rectangle (5, -7);
    \end{scope}

    \node[anchor=east] at (-1, 0)      {\texttt{energies}};
    \node[anchor=west] at (-8, -0.75)  {\small{Minimize (Listing~\ref{lst:minimize})}};
    \node[anchor=east] at (-1, -1.5)   {\texttt{is\_minimal}};
    \node[anchor=west] at (-8, -2.25)  {\small{Prefix Sum}};
    \node[anchor=east] at (-1, -3)     {\texttt{new\_index}};
    \node[anchor=east] at (-1, -4.5)   {\texttt{energies}};
    \node[anchor=west] at (-8, -5.5)   {\small{Shift minimal energies}};
    \node[anchor=east] at (-1, -6.5)   {$\mathrm{Min}$(\texttt{energies})};
\end{tikzpicture}
\caption{Compacting energies using a prefix sum.
    In this example, six threads each process a 3-dimensional energy tuple in parallel.
    Minimal energies are shaded.}%
\label{fig:compacting}
\end{figure}


\section{Data Layout}\label{sec:data}

An efficient implementation of the processes described above requires some
thought to be put into the data layout,
especially with the restrictive nature of the GPU architecture.
The highly parallelized execution model makes dynamic memory allocation very
unsuitable.
Instead, all required memory must be allocated before running a shader.

However in the case of the defend shader,
the exact amount of combinations that will be produced
and thus the required memory size cannot be predicted in advance.
To cope with that uncertainty,
each position is initially allocated memory for 64 energy tuples.
If while executing the shader this turns out to be insufficient,
the position is processed again later with double the amount of memory
until it succeeds.
In practice, however, 64 energy tuples is almost always enough.

Some of the input data for the shaders, such as the game graph,
remains static during the runtime of the algorithm
and only has to be sent to the GPU once.
Other data must be newly copied into GPU memory before every iteration of
shader calls.
This includes an array containing all the energies of the successors of
positions currently in the visit list.
Additional data structures tell,
where the entries for each position from the visit list begin,
and which energies belong to what successor.
The following two sections detail some of the techniques used to efficiently
store the data in memory.


\subsection{CSR Graph Format}

One major structure is the game graph,
which is required in the shaders for retrieving the energy updates.
It is a directed graph with an energy update associated to each edge.
One option for representing this graph would be an adjacency matrix,
where for any two positions it is saved if there is an edge between them,
along with the energy update for that edge.
This matrix would require $|G|^2$ entries and, since the game graph is
relatively sparse, the majority of those entries would be empty.
Such a high memory overhead would not make for a scalable implementation,
especially since the game graphs can easily get very large.
A common way of more efficiently storing graphs or sparse matrices in general is
the \emph{compressed sparse row} format (CSR)~\cite{Merrill2015,Hijma2023}.

The CSR format consists of three flat arrays.
The array $C$ holds all \emph{column-indices} of non-empty matrix cells in order.
It can be thought of as the concatenation of all adjacency lists for our graph.
Another array $R$ holds the \emph{row-offsets}, meaning the positions in array
$C$ where each row of the matrix starts.
Additional data for each edge can be stored in another array $W$, which has the
same length as $C$.
We use this third array for storing the energy updates.
Figure~\ref{fig:csr} illustrates this format on a small example graph.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}[position/.style={circle,draw}]
    \node[position] (0) {0};
    \node[position] (1) [right=of 0] {1}
        edge [<-] node[above] {$u_0$} (0);
    \node[position] (2) [below=of 0] {2}
        edge [<-] node[auto] {$u_1$} (0)
        edge [<-] node[near end,above] {$u_3$} (1);
    \node[position] (3) [below=of 1] {3}
        edge [<-] node[near end,above] {$u_2$} (0)
        edge [->] node[right] (u4) {$u_4$} (1)
        edge [->] node[auto] {$u_5$} (2);
    \node[right=6mm of u4] (matrix) {
        $\begin{bmatrix}
            - & u_0 & u_1 & u_2 \\
            - & -   & u_3 & -   \\
            - & -   & -   & -   \\
            - & u_4 & u_5 & -
        \end{bmatrix}$
    };
    \node[right=6mm of matrix] {
        $\begin{aligned}
            C &= [1, 2, 3, 2, 1, 2]\\
            R &= [0, 3, 4, 4, 6]\\
            W &= [u_0, u_1, u_2, u_3, u_4, u_5]
        \end{aligned}$
    };
\end{tikzpicture}
\end{center}
\caption{Example graph with adjacency matrix and CSR representation.
    $u_0, \ldots, u_5 \in \mathbf{Up}$ are the energy updates.
}%
\label{fig:csr}
\end{figure}

The CSR-format is very efficient in terms of memory usage and access,
but its main drawback is that inserting or deleting edges and nodes is very
expensive, since it requires shifting large parts of the arrays.
This is not a problem in our case, since we generate the game graph only once
in the beginning and then never change it.
Even though this format was chosen primarily with regards to the needs
of the GPU code, it also turned out to be very valuable to store the graph in
the same format in the host memory of the CPU\@.


\subsection{Storing Energy Sets}\label{subsec:energy_memory}

While running the algorithm, we need to keep track of a set of energy tuples
for each position in the game graph.
This corresponds to the \Energies array of Algorithm~\ref{alg:energy_game}.
The algorithm ensures that only minimized sets are stored,
meaning we do not use up more memory than necessary to represent the
upward-closed sets.
An alternative representation of upward-closed sets has been proposed
by \textcite{Delzanno2000},
where the minimal tuples are generated by the possible
paths through a specially constructed graph.
While that representation might allow
for more optimized memory usage in some cases,
it did not seem worth the additional complexity
of having to handle a graph structure.
Particularly with regards to GPU programming, a flat list of tuples is much
easier to work with.

In the case of this spectroscopy algorithm, we can significantly reduce the
size of the energy sets by constraining the range of the energy values.
Ultimately, we want to compare the winning budgets with the energy levels
corresponding to the different notions of equivalence.
However, these energy levels do not require any distinction of values above 2:
we can consider any values of 3 or above to be infinite without losing
information about the equivalences that we care about.
We can thus restrict the values of the energy tuples to $\{0, 1, 2, \infty\}$,
with updates saturating at $\infty$.

For once, this gives an upper bound to the size of antichains that might
possibly need to be stored as minimal sets.
Secondly, the 4 possible values can be stored in just 2 bits of memory
($\infty$ is represented by 3, or binary $11_2$).
The 6-tuples required for the spectroscopy algorithm can thus be bit-packed
into 12 bits.
The shader language WGSL includes only very few data types compared to most CPU
programming languages.
The only integer data types are the signed (\texttt{i32}) and unsigned
(\texttt{u32}) 32-bit integers,
so each energy tuple is stored in a \texttt{u32},
which could hold tuples with up to 16 values that are capped at 3.
Listing~\ref{lst:less_eq} shows how these bit-packed energies can
be worked with in shader code to test the componentwise $\leq$-relation.
Equality between two energies can be directly tested by comparing the two
\texttt{u32}s,
as is done in line~7 of Listing~\ref{lst:minimize}.

\begin{lstlisting}[language=WGSL,float,
    caption={WGSL Shader Code handling bit-packed energies
        to calculate if $a \leq b$.
        This is only for the case of 6 elements with a maximum value of 3,
        other configurations will produce different shader code.
        The bitwise AND operation \texttt{a\;\&\;0x3} sets all but the 2
        least significant bits to 0,
        thus comparing just the first field.
        This is repeated with all six fields,
        where each bit mask is written in hexadecimal format.
        For example, hexadecimal C0$_{16}$ is $\ldots0\,1100\,0000_2$ in
        binary, selecting the fourth 2-bit field.
    },
    label=lst:less_eq]
fn less_eq(a: u32, b: u32) -> bool {
    return (
        (a & 0x3)   <= (b & 0x3)   &&
        (a & 0xc)   <= (b & 0xc)   &&
        (a & 0x30)  <= (b & 0x30)  &&
        (a & 0xc0)  <= (b & 0xc0)  &&
        (a & 0x300) <= (b & 0x300) &&
        (a & 0xc00) <= (b & 0xc00)
    );
}
\end{lstlisting}

Similarly, update tuples $(u_1, \ldots, u_6) \in \mathbf{Up}_6$
for the spectroscopy algorithm can also be packed into a single \texttt{u32}.
According to Definition~\ref{def:update}, each update value $u_k$ can be either
no update at all (0),
a decrement update ($-1$),
or a $\minupd$-update.
At this point, we constrain ourselves to $\minupd$-updates $\minupd_D$
where $|D| = 2$.
Since the index of the update value in the tuple, $k$,
must always be a member of $D$,
the only information we really need is the index of the other element,
$D \setminus \{k\}$.
Together with the two options for $0$ and $-1$,
for $N$-dimensional updates this comes out to $N + 2$ distinct update values.
In our case of 6 dimensions, we need to represent 8 values,
which can fit in 3 bits.
For better alignment we choose the next power
of 2 and set aside 4 bits for each update field.
The exact binary representation for an update value $u_k$ is shown in
Table~\ref{tab:update_encoding}.

\begin{table}[ht]
\centering
\caption{Binary encoding of updates. $k$ is the index of this update value in
the tuple.}\label{tab:update_encoding}
\begin{tabular}{ r c l }
    \toprule
    Decimal & Binary & Update \\
    \midrule
    0 & $0000_2$ & 0 \emph{(No update)} \\
    1 & $0001_2$ & -1 \\
    2 & $0010_2$ & $\minupd_{k, 1}$ \\
    3 & $0011_2$ & $\minupd_{k, 2}$ \\
    \vdots & \vdots & \vdots \\
    7 & $0111_2$ & $\minupd_{k, 6}$ \\
    \bottomrule
\end{tabular}
\end{table}

While this section focused on 6-dimensional energies with maximal values of 3,
the implementation also supports more general energy games with arbitrary
dimensionality and value range by doing shader preprocessing.


\section{Limitations}

The high complexity of the spectroscopy algorithm,
both in runtime and memory usage,
stems from having to handle the energy sets
and also just the sheer size of the game graph,
which is usually significantly larger than the underlying transition system.
The shaders take care of updating and combining energy sets,
which makes up the \enquote{innermost loop} of the algorithm,
in a highly parallelized fashion. 
But a fair amount of work still has to be done
outside of that by the CPU,
which could not be efficiently parallelized for various reasons.

Ideally, most of the data needed during the algorithm would stay on the GPU,
so that less data needs to be copied between GPU and CPU memory.
But currently, the energy sets for each position reside in CPU memory.
Only the energies needed in the current iteration are sent over to the GPU,
and the resulting energy sets are then read back and saved.
This distribution of tasks is outlined in Figure~\ref{fig:attack}.
Keeping the energy sets in GPU memory instead would require some sort of
dynamic memory allocation,
since the size of the energy sets can grow almost arbitrarily
and the order in which energy sets are changed depends on the structure of the
graph.
Some approaches exist to handle dynamic data like that on a GPU,
for example the \emph{Hornet} data structure by \textcite{Busato2018}.
But its high design complexity puts that out of scope for this work.

Due to the size of the game graph, the process of generating it based on an
input LTS is also a tempting point for optimization.
But for similar reasons, this process had to be fully done on the CPU\@.
The generated graph is again a highly irregular data structure and
even parallelizing only the most critical parts could not achieve a speedup.
The fact that the positions themselves contain arbitrarily-sized sets of
processes doesn't help either.
In a test of only generating a large number of defense positions,
the shader took noticeably more time than a pure CPU-implementation of the same
process.

\subsection{Hardware Limits}\label{subsec:hw_limits}

Even though modern graphics cards can achieve truly incredible degrees of
parallelization,
there are still some hardware limits that we need to consider.
We cannot spawn arbitrarily many threads at once.
Most current GPUs have a limit of 65535 workgroups.
With 64 threads per workgroup this amounts to about 4.2 million threads,
which is the limit of what can be scheduled at once, the driver will most
likely not execute all threads simultaneously.

This means that on very large inputs,
we may not be able to process the entire visit list in a single shader call.
Instead, only part of the visit list may be processed at once,
taking into account the thread limit as well as memory usage.
The algorithm is still correct,
even if only some subset of the visit list is processed.
This is because the outcome of processing a position is only dependent
on the energies of its successors.
Whenever any of its successors gets changed,
the position is again added to the visit list.
Processing positions in a different order may therefore cause some of them
to be processed more often,
but the final result will not be affected.

Memory availability is another limiting factor.
The larger transition systems from the VLTS benchmarking suite~\cite{vlts}
caused the game graph generation to run out of memory
on a system with 32GB RAM\@.
But even if CPU and GPU both had sufficient memory,
in many situations the size of a single GPU buffer allocation
is limited to $2^{31} - 1$ bytes or 2GiB,
the maximum range of a signed 32-bit integer.
The largest allocation is required for the game graph,
where the energy updates require 4 bytes per edge.
This limits the size of the game graph to
$2^{29} - 1 = 536\,870\,911$ edges.
Based on the benchmarks shown in Chapter~\ref{ch:benchmarks},
processing such a maximally-sized graph would probably take around 4 minutes.
