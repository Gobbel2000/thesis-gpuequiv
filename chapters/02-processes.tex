With the goal of presenting the algorithm for finding equivalences between
processes,
this chapter will introduce the theoretical concepts that this work is based on.
First and foremost this includes an explanation and definition
of the models we use to represent processes.
Further, we will explore different ways to compare two processes and how that
leads us to the equivalence spectrum~\cite{glabbeek1990spectrum}.
Finally, we will outline the spectroscopy algorithm,
which gives us a multifaceted view of the relation of two processes
by testing all of those equivalences at once~\cite{bisping2023process}.


\section{Modeling Processes}

We represent processes in a directed graph.
The outgoing edges of a node are the possible steps it can perform next.
Each edge is labelled with a specific action that occurs when taking that step.
After performing an action,
the process enters a new state represented by another node,
which is itself a process with its set of actions.
This forms a sort of state machine that at every step
moves to some other state by performing one of the currently available actions.
The series of actions being chosen
constitutes the observable behavior of the machine,
while its current state defines what actions it can do next.
Such a structure is called a \emph{Labeled transition system}
or \emph{LTS} for short~\cite{reactive_systems}.
Figure~\ref{fig:example_lts} gives an example for what the graph of an LTS might
look like.

\begin{definition}[Labeled Transition System~\cite{reactive_systems}]
    A labeled transition system is a triple
    $(\mathsf{Proc}, \mathsf{Act}, {\rightarrow})$
    where:

    \begin{itemize}
        \item $\mathsf{Proc}$ is the set of states or processes,
        \item $\mathsf{Act}$ is a set of actions,
        \item ${\rightarrow} \subseteq \mathsf{Proc} \times \mathsf{Act} \times \mathsf{Proc}$
            is the transition relation.
            We also write
            $p \xrightarrow{\alpha} p'$ for $(p, \alpha, p') \in {\rightarrow}$,
            meaning we can transition from state $p$ to $p'$
            using the action $\alpha$.
    \end{itemize}
\end{definition}

\begin{figure}[tb]
\begin{center}
\begin{tikzpicture}[p/.style={circle,draw},->]
    \node[p] (0) {$p_0$}
        child {node[p] {$p_1$}
            child {node[p] {$p_2$} edge from parent node[left] {$b$}}
            child {node[p] {$p_3$} edge from parent node[right] {$c$}}
            edge from parent node[left] {$a$}
        };
    \node[p] (4) [right=25mm of 0] {$p_4$}
        child {node[p] {$p_5$}
            child {node[p] {$p_7$} edge from parent node[left] {$b$}}
            edge from parent node[left] {$a$}
        }
        child {node[p] {$p_6$}
            child {node[p] {$p_8$} edge from parent node[right] {$c$}}
            edge from parent node[right] {$a$}
        };
\end{tikzpicture}
\end{center}
\caption{An example for an LTS\@. In this case,
    $\mathsf{Proc} = \{p_0, p_1, \ldots, p_8\}$ and
    $\mathsf{Act} = \{a, b, c\}$.
}%
\label{fig:example_lts}
\end{figure}


\section{Equivalences Between Processes}

One of the most important questions that arises in the context of transition
systems is whether two processes show the same behavior.
This is a difficult question
because there are many different ways to define
what \emph{equivalent behavior} means.
This section will present two important definitions: Trace Equivalence and
Bisimilarity.

\subsubsection{Trace Equivalence}

A simple way to qualify the behavior of a process is to look at the possible
strings of actions (traces) it can perform.
This is particularly apparent when viewing an LTS as a sort of automaton.
A sequence of actions $\alpha_1 \ldots \alpha_k \in \mathsf{Act}^*$
is a trace of a process $p$ if
$p \xrightarrow{\alpha_1} p_1 \xrightarrow{\alpha_2} p_2
\ldots p_{k-1} \xrightarrow{\alpha_k} p_k$.
The function $\mathit{Traces}(p)$ produces the set of all traces
for a process $p$.
We now call two processes $p$ and $q$ \emph{trace-equivalent}
if $\mathit{Traces}(p) = \mathit{Traces}(q)$~\cite{reactive_systems}.

For example, in Figure~\ref{fig:example_lts} the traces of $p_0$ are
$\mathit{Traces}(p_0) = \{a, ab, ac\}$.
The process $p_4$ can produce exactly the same traces,
so $p_0$ and $p_4$ are trace-equivalent.
However it is not difficult to see some difference in the graphs of both
processes that this comparison seems to ignore.
Indeed, trace equivalence is one of the weakest equivalence notions,
meaning it more easily equates processes.
An even weaker equivalence would be \emph{enabledness},
which already equates two processes
if they have the same set of enabled actions,
given as
$\mathcal{I}(p) \colonequals \{\alpha \in \mathsf{Act} \mid
    {\exists p' \in \mathsf{Proc}}\,.\,p \xrightarrow{\alpha} p'\}$
for a process $p$.

Trace equivalence, as well as many other equivalences,
can also be seen directionally.
If one only cares about whether all traces of $p$ are also traces in $q$ and
not the other way around---meaning
$\mathit{Traces}(p) \subseteq \mathit{Traces}(q)$---the result is a
preorder instead of an equivalence relation where $p$ is preordered by $q$
if the condition holds.


\subsubsection{Bisimilarity}

\emph{Bisimilarity} is special because it is the strongest of the behavioral
equivalences.
It requires two processes to provide the same set of actions not only initially,
but also after every time both processes have been advanced
by the same chain of actions.
It is defined as follows:

\begin{definition}[Bisimulation~\cite{reactive_systems}]\label{def:bisimulation}
    Processes $p, q \in \mathsf{Proc}$ are bisimilar, written $p \sim q$,
    if and only if there exists a relation
    $\mathcal{R} \subseteq \mathsf{Proc} \times \mathsf{Proc}$
    with $p\, \mathcal{R}\, q$ and for every $s_1\, \mathcal{R}\, s_2$ and
    $\alpha \in \mathsf{Act}$:
    \begin{itemize}
        \item if $s_1 \xrightarrow{\alpha} s_1'$, then there is a transition
            $s_2 \xrightarrow{\alpha} s_2'$ such that $s_1'\, \mathcal{R}\, s_2'$,
        \item if $s_2 \xrightarrow{\alpha} s_2'$, then there is a transition
            $s_1 \xrightarrow{\alpha} s_1'$ such that $s_1'\, \mathcal{R}\, s_2'$.
    \end{itemize}
    A relation $\mathcal{R}$ satisfying these conditions is called a
    \emph{bisimulation}.
\end{definition}

This does not hold for our example in Figure~\ref{fig:example_lts}.
If we were trying to construct a bisimulation relation with
$p_0\, \mathcal{R}\, p_4$,
that would require $p_1\, \mathcal{R}\, p_5$
because $p_0 \xrightarrow{a} p_1$ and $p_4 \xrightarrow{a} p_5$.
But $p_1$ has a $c$-transition to $p_3$ which $p_5$ cannot simulate,
thus there is no bisimulation that relates $p_0$ and $p_4$.


\subsection{Hennessy--Milner Logic}

A very useful tool when discussing process equivalences is the
Hennessy--Milner logic (or HML),
which is a modal logic that can be used to formalize various
properties of a process.
A process satisfies a HML-formula if it observes
the property specified by the formula,
otherwise it does not satisfy it.
We adopt the syntax from~\cite{bisping2023process},
which is slightly different although functionally equivalent to the definition
in~\cite{reactive_systems}.
This is relevant because we will later require precise control over the
structure of HML-formulas.

\begin{definition}[%
    Hennessy--Milner logic~\cite{bisping2023process,reactive_systems}]%
    \label{def:hml}
    The set $\mathcal{M}$ of HML-formulas over a set of actions $\mathsf{Act}$
    is constructed by the following grammar:
    \begin{align*}
        \varphi \coloncolonequals\ &\langle \alpha \rangle \varphi,
            \qquad \alpha \in \mathsf{Act} \\
            \mid \quad &\bigwedge \{\psi, \psi, \ldots\} \\
        \psi \coloncolonequals\ &\neg \varphi \mid \varphi
    \end{align*}
    The semantics $\llbracket \varphi \rrbracket$ of a formula
    $\varphi$ in a given LTS
    $(\mathsf{Proc}, \mathsf{Act}, {\rightarrow})$ denotes the set of processes in
    $\mathsf{Proc}$ that satisfy $\varphi$.
    It is recursively defined as:
    \begin{align*}
        \llbracket \langle \alpha \rangle \varphi \rrbracket &\colonequals
            \{p \in \mathsf{Proc} \mid
              \exists p' \in \llbracket \varphi \rrbracket\,.\,
              p \xrightarrow{\alpha} p'
            \} \\
        \llbracket \neg \varphi \rrbracket &\colonequals
            \mathsf{Proc} \setminus \llbracket \varphi \rrbracket \\
        \llbracket \bigwedge_{i \in I} \psi_i \rrbracket &\colonequals
            \bigcap_{i \in I} \llbracket \psi_i \rrbracket
    \end{align*}
    We use $\llbracket \neg \varphi \rrbracket$,
    but $\neg \varphi$ is not a member of $\mathcal{M}$:
    negated formulas can only occur inside a conjunction.
    The recursion in the above definition can be terminated using the empty
    conjunction $\top \colonequals \bigwedge \{\}$
    which itself always evaluates to true:
    $\llbracket \top \rrbracket = \mathsf{Proc}$.

    For $p \in \mathsf{Proc},\, \varphi \in \mathcal{M}$
    we write $p \models \varphi$ if, and only if,
    $p \in \llbracket \varphi \rrbracket$,
    meaning $p$ satisfies the formula $\varphi$.
\end{definition}

To give a few examples,
both $p_0$ and $p_4$ of Figure~\ref{fig:example_lts} satisfy the HML-formula
$\langle a \rangle \langle b \rangle \top$,
which requires an $a$-transition followed by a $b$-transition,
thus encoding the trace $ab$.
However the formula
$\varphi = \langle a \rangle \bigwedge \{
    \langle b \rangle \top, \langle c \rangle \top \}$
is only satisfied by $p_0$, not by $p_4$,
since it requires an $a$-transition
after which both actions $b$ and $c$ should be possible.
$p_4$ does have two $a$-transitions,
but neither $p_5$ nor $p_6$ offers both $b$ and $c$.
Therefore, $\varphi$ is a \emph{distinguishing formula} for $p_0$ and $p_4$,
since it points out a difference in their behavior.


\subsection{The Equivalence Spectrum}

A number of behavioral equivalences have been collected and ordered
in the \emph{linear-time--branching-time spectrum}
by \textcite{glabbeek1990spectrum}.
By examining subsets of HML-formulas we can achieve a general definition
for all equivalences,
which is a crucial step towards our goal of testing all of them at once.

Each notion of equivalence is characterized by a subset of HML-formulas
$\mathcal{O} \subseteq \mathcal{M}$.
If the subset includes a distinguishing formula for two processes $p$ and $q$,
$\exists \varphi \in \mathcal{O}\,.\,
    p \models \varphi \wedge q \not\models \varphi$,
that equivalence does not hold, it is refuted by $\varphi$.
If there is no such formula,
$p$ is preordered by $q$ with respect to the chosen equivalence.
Only if $q$ is also preordered by $p$,
meaning there is also no distinguishing formula $\varphi' \in \mathcal{O}$
with $q \models \varphi'$ and $p \not\models \varphi'$,
can we call $p$ and $q$ equivalent with respect to the chosen set of formulas.

\textcite{bisping2023process} represents these subsets of $\mathcal{M}$
by assigning a 6-dimensional \enquote{expressiveness price} to each formula
and construcing the subsets by including all formulas below a maximum price.
Without going further into the exact definition of the prices,
all equivalences, along with their price vector representing a subset of
HML-formulas are listed in Table~\ref{tab:spectrum}.

\begin{table}[htpb]
    \centering
    \caption{Equivalences of the linear-time--branching-time spectrum and the
        maximum expressiveness a HML-formula may have to refute this
        equivalence~\cite{bisping2023process}.}%
    \label{tab:spectrum}
    \begin{tabular}{l l}
        \toprule
        Equivalence &Price \\
        \midrule
        Bisimulation &$(\infty, \infty, \infty, \infty, \infty, \infty)$ \\
        2-nested Simulation &$(\infty, \infty, \infty, \infty, \infty, 1)$ \\
        Ready Simulation &$(\infty, \infty, \infty, \infty, 1, 1)$ \\
        Readiness Traces &$(\infty, \infty, \infty, 1, 1, 1)$ \\
        Possible Futures &$(\infty, 2, \infty, \infty, \infty, 1)$ \\
        Simulation &$(\infty, \infty, \infty, \infty, 0, 0)$ \\
        Failure Traces &$(\infty, \infty, \infty, 0, 1, 1)$ \\
        Readiness &$(\infty, 2, 1, 1, 1, 1)$ \\
        Revivals &$(\infty, 2, 1, 0, 1, 1)$ \\
        Impossible Futures &$(\infty, 2, 0, 0, \infty, 1)$ \\
        Failures &$(\infty, 2, 0, 0, 1, 1)$ \\
        Traces &$(\infty, 1, 0, 0, 0, 0)$ \\
        Enabledness &$(1, 1, 0, 0, 0, 0)$ \\
        \bottomrule
    \end{tabular}
\end{table}

The strongest equivalence, bisimulation, puts no restrictions on the price of
distinguishing formulas: all components are set to $\infty$.
This property is famously characterized by the Hennessy--Milner-theorem,
stating that two processes are bisimilar if, and only if,
they satisfy exactly the same HML-formulas~\cite{reactive_systems}.


\section{Energy Games}\label{sec:energy_games}

The question of finding equivalences can now be reformulated to ask instead
about the costs required to distinguish two processes.
Once this is known, the set of satisfied equivalences can be derived from
Table~\ref{tab:spectrum}.
These costs are computed with the help of an \emph{energy game},
which is an extension of the classic \emph{reachability game}.

In a reachability game,
the goal of the attacker player is to reach a predefined set of target positions,
while the defender tries to prevent it.
The energy game simply adds the restriction that the attacker must also start
out with a sufficient \emph{energy budget},
which is decreased at every step.
In order to support multiple, independent cost factors,
these energies can be multidimensional:

\begin{definition}[Energies~\cite{bisping2023process,brihaye2023multi}]%
    \label{def:energies}
    The set of $N$-dimensional energies is given as
    \[\mathbf{En}_N \colonequals {(\mathbb{N} \cup \{ \infty \})}^N.\]
    Energy tuples are compared component-wise, which creates a partial order:
    \[(e_1, \ldots, e_N) \leq (f_1, \ldots, f_N) \; \Leftrightarrow \;
        e_i \leq f_i \text{ for all } i \in [N].\]
    The supremum between two energy tuples is defined as usual:
    \[\sup((e_1, \ldots, e_N), (f_1, \ldots, f_N)) \colonequals
        (\max(e_1, f_1), \ldots, \max(e_N, f_N)).\]
    The upward closure $\upclosed E$ for a set of energies
    $E \subseteq \mathbf{En}$ contains all energies that are greater or equal
    to an element in $E$:
    \[\upclosed E \colonequals
        \{e \in \mathbf{En} \mid \exists e' \in E\,.\,e \geq e'\}\]
    Conversely, we can reduce an energy set $E \subseteq \mathbf{En}$
    to its minimal elements:
    \[\mathrm{Min} (E) \colonequals
        \{e \in E \mid \nexists e' \in E\,.\,e' \leq e \wedge e' \ne e\}\]
\end{definition}

We will construct the energy game in such a way that the attacker essentially
aims to point out a distinction between two processes,
while the energy budget required by the attacker could be seen as the
expressiveness cost of a formula encoding that distinction.
The prices in Table~\ref{tab:spectrum} are elements of $\mathbf{En}_6$ and
bring us back from energies to equivalences.

When discussing the energy game, it will be convenient to consider
upward-closed energy sets $\upclosed E$,
but when the energies are based on the natural numbers,
these sets are obviously infinite in size.
In practice we only store minimized energy sets $\mathrm{Min}(E)$.
Because all energies in a minimized set are incomparable to each other,
they form an antichain.
Such an antichain, representing a larger upward-closed set,
is also referred to as a \emph{Pareto frontier}~\cite{brihaye2023multi}.

In the regular energy game as presented by \textcite{brihaye2023multi},
energies are simply reduced by subtracting another vector of natural numbers.
However for the purpose of the spectroscopy algorithm,
we require a special update function that includes $\minupd$-updates.
It is important to note that with this definition energies are only ever
\emph{decreased} by an update:
$\mathsf{upd}(e, u) \leq e$ for all $u \in \mathbf{Up}$.
The following definitions closely follow \textcite{bisping2023process},
who first proposed the usage of energy games for equivalence analysis.

\begin{definition}[Energy Updates~\cite{bisping2023process}]\label{def:update}
    The set of $N$-dimensional energy updates $\mathbf{Up}_N$ contains
    $N$-tuples $(u_1, \ldots, u_N) \in \mathbf{Up}_N$ where each update $u_k, k
    \in [N]$ is
    either
    \begin{itemize}
        \item $u_k \in \{-1, 0\}$, or
        \item $u_k = \minupd_D$ where $D \subseteq [N]$ and $k \in D$.
    \end{itemize}

    The partial function
    $\mathsf{upd}_N: (\mathbf{En}_N, \mathbf{Up}_N) \rightharpoonup \mathbf{En}_N$
    applies an update to an energy tuple.
    The $k$-th component is given as:
    \begin{equation*}
        \mathsf{upd}_N{(e, u)}_k =
        \begin{cases}
            e_k + u_k,\quad &\text{if } u_k \in \{-1, 0\} \text{ and } e_k \geq 1, \\
            \min_{d \in D}{e_d},\quad &\text{if } u_k = \minupd_D.
        \end{cases}
    \end{equation*}
\end{definition}

\begin{definition}[Energy Games~\cite{bisping2023process}]\label{def:energy_game}
    We define an $N$-dimensional energy game as
    $(G, G_a, G_d, E, w, g_0, e_0)$, where
    \begin{itemize}
        \item $G$ is a set of game positions.
        \item $G_a$ and $G_d$ are the sets of attacker and defender positions
            respectively.
            $G = G_a \cup G_d$ and $G_a \cap G_d = \emptyset$.
        \item $E \subseteq (G \times G)$ is the edge relation. $(G, E)$
            together form a directed game graph.
        \item $w: E \rightarrow \mathbf{Up}_N$ is a weight function, assigning an
            energy update to each edge.
        \item $g_0 \in G$ is the start position of the game.
        \item $e_0 \in \mathbf{En}_N$ is the starting energy budget of the attacker.
    \end{itemize}

    The function $\mathrm{Succ}: G \rightarrow 2^G$ gives the successors in the
    graph for a position.
    For each $g \in G$, $\mathrm{Succ}(g) = \{g' \in G \mid (g, g') \in E\}$.
    Similarly, $\mathrm{Pred}(g) = \{g' \in G \mid (g', g) \in E\}$ yields a
    position's predecessors.
\end{definition}

\begin{definition}[Plays and Costs~\cite{bisping2023process}]
    A \emph{play} $\rho$ on a given game is a sequence of positions:
    $\rho = g_0g_1 \ldots g_n$ where at each step
    $g_i \in G$ and $(g_i, g_{i+1}) \in E$.

    The energy level at step $i$ in the play $\rho$, $\mathsf{EL}_\rho(i)$,
    is initially $\mathsf{EL}_\rho(0) \colonequals e_0$.
    After each step the energy level is updated:
    $\mathsf{EL}_\rho(i + 1) \colonequals
        \mathsf{upd}(\mathsf{EL}_\rho(i), w(g_i, g_{i+1}))$.
    A step from $g_i$ to $g_{i+1}$ is not allowed
    if there is not enough energy left for it,
    in which case $\mathsf{upd}(\mathsf{EL}_\rho(i), w(g_i, g_{i+1}))$ is undefined.

    If a play cannot be further extended,
    that is, either the energy was depleted or
    $\mathrm{Succ}(g_n) = \emptyset$,
    this play is won by the player who is not stuck:
    if $g_n \in G_a$, the defender wins, if $g_n \in G_d$, the attacker wins.
    Infinite plays are won by the defender.
\end{definition}

The problem we want to investigate now is
what the minimum required energies are
in order for the attacker to win the game,
regardless of the defender's decisions.
For a starting point $g_0 \in G$ we denote this set of winning budgets as
$W(g_0) \subseteq \mathbf{En}$.
If we find one energy tuple $e \in W(g_0)$ that is sufficient to win the game,
we know that all greater energies $e' \geq e$ must also lie in $W(g_0)$.
Therefore, $W(g_0)$ is an upward-closed set and we will often just consider
its minimal elements $\mathrm{Min}(W(g_0))$.
Exactly this task of finding the minimal winning budgets in an energy game
is at the center of what this work tackles with the help of GPU-acceleration,
and will be further elaborated in Chapter~\ref{ch:implementation}.


\section{Spectroscopy Algorithm}\label{sec:spectroscopy}

The entire spectroscopy algorithm can now be outlined by
first generating an energy game for a given LTS,
and then computing the winning budgets for that particular game.
These winning budgets then represent the costs required to distinguish a
process pair,
which can be translated into some of the common equivalence notions.
The part that still remains is how to construct the energy game
so it actually serves to inform us about distinctions between processes.
Here, it will suffice to briefly recapitulate the rules that are used to
construct the game graph.
A more extensive justification for them is provided in~\cite{bisping2023process}.

The game graph for an transition system
$(\mathsf{Proc}, \mathsf{Act}, {\rightarrow})$
is constructed around three types of game positions:

\begin{itemize}
    \item Attacker positions ${(p, Q)}_a \in G_a$
    \item Attacker clause positions ${(p, q)}_a^{\scriptscriptstyle\land} \in G_a$
    \item Defender positions ${(p, Q, Q_*)}_d \in G_d$
\end{itemize}

$p, q \in \mathsf{Proc}$ and $Q, Q_* \subseteq \mathsf{Proc}$.
The graph is induced by the following rules,
which define the edge relation $E$, as well as the update weights $w$.
The transition over sets $Q \xrightarrow{\alpha} Q'$
is defined as
$Q' = \{q' \in \mathsf{Proc} \mid
    \exists q \in Q\,.\,q \xrightarrow{\alpha} q'\}$.


\begin{table}[h!]
\centering

\begin{tabular}{l l l l}
    \toprule
    Position &Update &Next Position &Conditions \\
    \midrule
    ${(p, Q)}_a$ &$(-1, 0, 0, 0, 0, 0)$ &${(p', Q')}_a$
        &$p \xrightarrow{\alpha} p', Q \xrightarrow{\alpha} Q'$, \\
      &&&$p' \notin Q', Q \neq \emptyset$ \\
    ${(p, Q)}_a$ &$(0, -1, 0, 0, 0, 0)$ &${(p, Q \setminus Q_*, Q_*)}_d$
        &$Q_* \subsetneq Q, Q_* \in \mathcal{Q}$ \\
    ${(p, Q, Q_*)}_d$ &$(\minupd_{\{1, 3\}}, 0, 0, 0, 0, 0)$ &${(p, Q_*)}_a$
        &$Q_* \neq \emptyset$ \\
    ${(p, Q, Q_*)}_d$ &$(0, 0, 0, \minupd_{\{3, 4\}}, 0, 0)$
        &${(p, q)}_a^{\scriptscriptstyle\land}$
        &for $q \in Q$ \\
    ${(p, q)}_a^{\scriptscriptstyle\land}$
        &$(\minupd_{\{1, 4\}}, 0, 0, 0, 0, 0)$
        &${(p, \{q\})}_a$ \\
    ${(p, q)}_a^{\scriptscriptstyle\land}$
        &$(\minupd_{\{1, 5\}}, 0, 0, 0, 0, -1)$
        &${(q, \{p\})}_a$ \\
    \bottomrule
\end{tabular}
\end{table}

These rules already contain some optimizations that reduce the final game graph
size without affecting the results.
Defender positions are limited to cases
${(p, Q \setminus Q_*, Q_*)}_d$ with
$Q_* \in \mathcal{Q} \colonequals
\big\{\emptyset,
    \{q \in Q \mid \mathcal{I}(p) \subseteq \mathcal{I}(q)\},
    \{q \in Q \mid {\mathcal{I}(p) \supseteq \mathcal{I}(q)}\},
    \{q \in Q \mid \mathcal{I}(p) =         \mathcal{I}(q)\}
\big\}$,
where $\mathcal{I}(p)$ is the set of enabled actions of $p$.
This restriction to at most four different partitions
based on the enabled actions is called the
\enquote{clever spectroscopy game} in~\cite{bisping2023process}.
Generating defender positions ${(p, \emptyset, Q)}_d$ from ${(p, Q)}_a$
is also not required,
because their only successor is ${(p, Q)}_a$ again.
These loops do not influence the final winning budgets,
therefore the partitions $Q_*$ are limited to strict subsets of $Q$.

The meaning behind an attacker position ${(p, Q)}_a$ is roughly
that the attacker wants to distinguish $p$ from all processes in $Q$.
The game graph is typically seeded with one or more attacker positions
in the form ${(p, \{q\})}_a$.
Starting from there, the graph is then recursively built up
using the rules above.
Finally, the winning budgets for a starting position,
$W({(p, \{q\})}_a)$,
are exactly the costs required to distinguish $p$ and $q$.

Subsequently, attacker positions ${(p, Q)}_a$ with $p \in Q$ can be left out
since it is impossible to distinguish $p$ from itself.
This means the attacker can never win starting from such a position,
thus not contributing any winning budgets to other positions either.
If an attacker position ${(p, Q)}_a$ contains the empty set $Q = \emptyset$,
the attacker can win straight away by transitioning to
${(p, \emptyset, \emptyset)}_d$, which has no further successors.
No better costs can be achieved by considering further observations from $p$
and transitioning to any position ${(p', \emptyset)}_a$.
%
%Knowing the relevant aspects of the spectroscopy algorithm,
%we can now turn towards realizing it in GPU code.
